{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import math\n",
    "from tqdm import trange\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "\n",
    "# Define transformations for training data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Define transformations for safety set (including augmentation)\n",
    "safety_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST('./data', train=False, transform=transform)\n",
    "# Split data into tensors\n",
    "X_train, Y_train = next(iter(torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=len(train_dataset))))\n",
    "X_test, Y_test = next(iter(torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=len(test_dataset))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(QConv2d, self).__init__()\n",
    "        self.kernel_size = (kernel_size, kernel_size) if isinstance(\n",
    "            kernel_size, int) else tuple(kernel_size)\n",
    "        scale = 1 / math.sqrt(in_channels * math.prod(self.kernel_size))\n",
    "        self.weight = nn.Parameter(torch.empty(\n",
    "            out_channels, in_channels, *self.kernel_size).uniform_(-scale, scale))\n",
    "        self.e = nn.Parameter(torch.full((out_channels, 1, 1, 1), -8.))\n",
    "        self.b = nn.Parameter(torch.full((out_channels, 1, 1, 1), 32.))  # Start with 32 bits\n",
    "\n",
    "    def qbits(self):\n",
    "        return self.b.relu().sum() * self.weight[0].numel()\n",
    "\n",
    "    def qweight(self):\n",
    "        b_rel = self.b.relu()\n",
    "        min_val = torch.where(b_rel > 0, -2 ** (b_rel - 1), torch.zeros_like(b_rel))\n",
    "        max_val = torch.where(b_rel > 0, 2 ** (b_rel - 1) - 1, torch.zeros_like(b_rel))\n",
    "        scaled_weight = 2 ** -self.e * self.weight\n",
    "        qweight = torch.max(torch.min(scaled_weight, max_val), min_val)\n",
    "        return qweight\n",
    "\n",
    "    def forward(self, x):\n",
    "        qw = self.qweight()\n",
    "        w = (qw.round() - qw).detach() + qw  # Straight-through estimator\n",
    "        return nn.functional.conv2d(x, 2 ** self.e * w)\n",
    "        \n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            QConv2d(1, 32, 5), nn.ReLU(),\n",
    "            QConv2d(32, 32, 5), nn.ReLU(),\n",
    "            nn.BatchNorm2d(32, affine=False, track_running_stats=False),\n",
    "            nn.MaxPool2d(2),\n",
    "            QConv2d(32, 64, 3), nn.ReLU(),\n",
    "            QConv2d(64, 64, 3), nn.ReLU(),\n",
    "            nn.BatchNorm2d(64, affine=False, track_running_stats=False),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Linear(64 * 3 * 3, 10)  # Adjusted to 64 * 3 * 3\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def qbits(self):\n",
    "        return sum(l.qbits() for l in self.features if isinstance(l, QConv2d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafetySetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, safety_set_path, transform=None):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        for file in os.listdir(safety_set_path):\n",
    "            if file.endswith('.png'):\n",
    "                label_str = file.split('_label_')[-1].split('.png')[0]\n",
    "                label = int(label_str)\n",
    "                image_path = os.path.join(safety_set_path, file)\n",
    "                self.image_paths.append(image_path)\n",
    "                self.labels.append(label)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transform(image)  # Default transform\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model().to(device)\n",
    "opt = optim.Adam(model.parameters())\n",
    "weight_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Load safety set\n",
    "safety_set_path = \"/home/mohammad/Research/Safety-Driven-Self-Compressing-Neural-Networks/safety_set_images_d\"  # Update this path if necessary\n",
    "safety_dataset = SafetySetDataset(safety_set_path, transform=safety_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    # Main training batch\n",
    "    samples = torch.randint(0, X_train.shape[0], (512,))\n",
    "    outputs = model(X_train[samples].to(device))\n",
    "    loss = nn.functional.cross_entropy(outputs, Y_train[samples].to(device))\n",
    "    Q = model.qbits() / weight_count\n",
    "    loss = loss + 0.1 * Q  # Hyperparameter determines compression vs accuracy\n",
    "\n",
    "    # Safety set penalty\n",
    "    # Randomly sample from safety dataset with augmentation\n",
    "    safety_indices = torch.randint(0, len(safety_dataset), (64,))\n",
    "    safety_images_batch = []\n",
    "    safety_labels_batch = []\n",
    "    for idx in safety_indices:\n",
    "        img, label = safety_dataset[idx]\n",
    "        safety_images_batch.append(img)\n",
    "        safety_labels_batch.append(label)\n",
    "    safety_images_batch = torch.stack(safety_images_batch).to(device)\n",
    "    safety_labels_batch = torch.tensor(safety_labels_batch).to(device)\n",
    "    safety_outputs = model(safety_images_batch)\n",
    "    safety_loss = nn.functional.cross_entropy(safety_outputs, safety_labels_batch)\n",
    "    safety_weight = 0.1  # Adjust this weight as needed\n",
    "    loss = loss + safety_weight * safety_loss\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    return loss.item(), Q.item(), safety_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_acc():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test.to(device))\n",
    "        pred = outputs.argmax(dim=1)\n",
    "        return (pred == Y_test.to(device)).float().mean().item() * 100\n",
    "\n",
    "# Safety set without augmentations for evaluation\n",
    "safety_dataset_eval = SafetySetDataset(safety_set_path, transform=transform)\n",
    "safety_loader_eval = torch.utils.data.DataLoader(safety_dataset_eval, batch_size=64, shuffle=False)\n",
    "\n",
    "def get_safety_acc():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in safety_loader_eval:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return (correct / total) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_zero_bit_kernels():\n",
    "    for layer in model.features:\n",
    "        if isinstance(layer, QConv2d):\n",
    "            if (layer.b.view(-1) <= 0).any():\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def restore_zero_bit_kernels(restore_fraction=0.5):\n",
    "    for layer in model.features:\n",
    "        if isinstance(layer, QConv2d):\n",
    "            b_flat = layer.b.view(-1)\n",
    "            zero_bit_indices = (b_flat <= 0).nonzero(as_tuple=False).view(-1)\n",
    "            num_restore = int(restore_fraction * len(zero_bit_indices))  # Restore half\n",
    "            if num_restore > 0:\n",
    "                restore_indices = zero_bit_indices[torch.randperm(len(zero_bit_indices))[:num_restore]]\n",
    "                b_flat[restore_indices] = 2.0  # Restore bits to 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for tracking\n",
    "prev_safety_acc = None\n",
    "safety_acc_drop_threshold = 5.0  # Percentage points\n",
    "test_accs, bytes_used, safety_losses = [], [], []\n",
    "\n",
    "# Initial safety accuracy\n",
    "initial_safety_acc = get_safety_acc()\n",
    "prev_safety_acc = initial_safety_acc\n",
    "\n",
    "# Training loop\n",
    "for i in (t := trange(4000)):\n",
    "    loss, Q, safety_loss = train_step()\n",
    "    model_bytes = Q / 8 * weight_count\n",
    "    if i % 10 == 9:\n",
    "        test_acc = get_test_acc()\n",
    "        safety_acc = get_safety_acc()\n",
    "        acc_drop = prev_safety_acc - safety_acc\n",
    "        if acc_drop > safety_acc_drop_threshold:\n",
    "            if check_zero_bit_kernels():\n",
    "                restore_zero_bit_kernels(restore_fraction=0.5)\n",
    "        prev_safety_acc = safety_acc\n",
    "    else:\n",
    "        test_acc = test_accs[-1] if test_accs else 0.0\n",
    "    test_accs.append(test_acc)\n",
    "    bytes_used.append(model_bytes)\n",
    "    safety_losses.append(safety_loss)\n",
    "    t.set_description(f\"loss: {loss:6.2f}  bytes: {model_bytes:.1f}  acc: {test_acc:5.2f}%  safety_loss: {safety_loss:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot test accuracy over iterations\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(test_accs, label='Test Accuracy')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Test Accuracy over Training Iterations')\n",
    "plt.legend()\n",
    "\n",
    "# Plot model size over iterations\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(bytes_used, label='Model Size (bytes)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Model Size (bytes)')\n",
    "plt.title('Model Size over Training Iterations')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model \n",
    "torch.save(model.state_dict(), 'modelquantizedfinal.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final metrics\n",
    "print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"Final Safety Accuracy: {safety_acc:.2f}%\")\n",
    "print(f\"Initial Safety Accuracy: {initial_safety_acc:.2f}%\")\n",
    "print(f\"Final Model Size: {model_bytes:.2f} bytes\")\n",
    "print(f\"Total Training Loss: {loss:.4f}\")\n",
    "print(f\"Total Q Bits: {Q:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
