{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import math\n",
    "from tqdm import trange\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "\n",
    "# Define transformations for training data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Define transformations for safety set (including augmentation)\n",
    "safety_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "# Split data into tensors\n",
    "X_train, Y_train = next(iter(torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=len(train_dataset))))\n",
    "X_test, Y_test = next(iter(torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=len(test_dataset))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the quantized convolutional layer\n",
    "class QConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(QConv2d, self).__init__()\n",
    "        self.kernel_size = (kernel_size, kernel_size) if isinstance(\n",
    "            kernel_size, int) else tuple(kernel_size)\n",
    "        scale = 1 / math.sqrt(in_channels * math.prod(self.kernel_size))\n",
    "        self.weight = nn.Parameter(torch.empty(\n",
    "            out_channels, in_channels, *self.kernel_size).uniform_(-scale, scale))\n",
    "        self.e = nn.Parameter(torch.full((out_channels, 1, 1, 1), -8.))\n",
    "        self.b = nn.Parameter(torch.full((out_channels, 1, 1, 1), 32.))  # Start with 32 bits\n",
    "\n",
    "    def qbits(self):\n",
    "        return self.b.relu().sum() * self.weight[0].numel()\n",
    "\n",
    "    def qweight(self):\n",
    "        b_rel = self.b.relu()\n",
    "        min_val = torch.where(b_rel > 0, -2 ** (b_rel - 1), torch.zeros_like(b_rel))\n",
    "        max_val = torch.where(b_rel > 0, 2 ** (b_rel - 1) - 1, torch.zeros_like(b_rel))\n",
    "        scaled_weight = 2 ** -self.e * self.weight\n",
    "        qweight = torch.max(torch.min(scaled_weight, max_val), min_val)\n",
    "        return qweight\n",
    "\n",
    "    def forward(self, x):\n",
    "        qw = self.qweight()\n",
    "        w = (qw.round() - qw).detach() + qw  # Straight-through estimator\n",
    "        return nn.functional.conv2d(x, 2 ** self.e * w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model using quantized convolutional layers\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            QConv2d(1, 32, 5), nn.ReLU(),\n",
    "            QConv2d(32, 32, 5), nn.ReLU(),\n",
    "            nn.BatchNorm2d(32, affine=False, track_running_stats=False),\n",
    "            nn.MaxPool2d(2),\n",
    "            QConv2d(32, 64, 3), nn.ReLU(),\n",
    "            QConv2d(64, 64, 3), nn.ReLU(),\n",
    "            nn.BatchNorm2d(64, affine=False, track_running_stats=False),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Linear(64 * 3 * 3, 10)  # Adjusted to 64 * 3 * 3\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def qbits(self):\n",
    "        return sum(l.qbits() for l in self.features if isinstance(l, QConv2d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SafetySetDataset class\n",
    "class SafetySetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, safety_set_path, transform=None):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        for file in os.listdir(safety_set_path):\n",
    "            if file.endswith('.png'):\n",
    "                label_str = file.split('_label_')[-1].split('.png')[0]\n",
    "                label = int(label_str)\n",
    "                image_path = os.path.join(safety_set_path, file)\n",
    "                self.image_paths.append(image_path)\n",
    "                self.labels.append(label)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transform(image)  # Default transform\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model and define optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model().to(device)\n",
    "opt = optim.Adam(model.parameters())\n",
    "weight_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Load safety set\n",
    "safety_set_path = \"/home/mohammad/Safety-Driven-Self-Compressing-Neural-Networks/Computer Vision/XAI/safety_set_images_d\"  # Update this path if necessary\n",
    "safety_dataset = SafetySetDataset(safety_set_path, transform=safety_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training step function with increased compression weight\n",
    "def train_step():\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    # Main training batch\n",
    "    samples = torch.randint(0, X_train.shape[0], (512,))\n",
    "    outputs = model(X_train[samples].to(device))\n",
    "    loss = nn.functional.cross_entropy(outputs, Y_train[samples].to(device))\n",
    "    Q = model.qbits() / weight_count\n",
    "    \n",
    "    # **Increased compression regularization weight from 0.05 to 0.1**\n",
    "    compression_weight = 0.1  # Adjusted from 0.05 to 0.1\n",
    "    loss = loss + compression_weight * Q  # Hyperparameter determines compression vs accuracy\n",
    "\n",
    "    # Safety set penalty\n",
    "    # Randomly sample from safety dataset with augmentation\n",
    "    safety_indices = torch.randint(0, len(safety_dataset), (64,))\n",
    "    safety_images_batch = []\n",
    "    safety_labels_batch = []\n",
    "    for idx in safety_indices:\n",
    "        img, label = safety_dataset[idx]\n",
    "        safety_images_batch.append(img)\n",
    "        safety_labels_batch.append(label)\n",
    "    safety_images_batch = torch.stack(safety_images_batch).to(device)\n",
    "    safety_labels_batch = torch.tensor(safety_labels_batch).to(device)\n",
    "    safety_outputs = model(safety_images_batch)\n",
    "    safety_loss = nn.functional.cross_entropy(safety_outputs, safety_labels_batch)\n",
    "    \n",
    "    # **Adjusted safety loss weight to balance compression and safety**\n",
    "    safety_weight = 0.05  # Reduced from 0.1 to 0.05\n",
    "    loss = loss + safety_weight * safety_loss\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    return loss.item(), Q.item(), safety_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to get test and safety accuracies\n",
    "def get_test_acc():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test.to(device))\n",
    "        pred = outputs.argmax(dim=1)\n",
    "        return (pred == Y_test.to(device)).float().mean().item() * 100\n",
    "\n",
    "# Safety set without augmentations for evaluation\n",
    "safety_dataset_eval = SafetySetDataset(safety_set_path, transform=transform)\n",
    "safety_loader_eval = torch.utils.data.DataLoader(safety_dataset_eval, batch_size=64, shuffle=False)\n",
    "\n",
    "def get_safety_acc():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in safety_loader_eval:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return (correct / total) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to check and restore zero-bit kernels\n",
    "def check_zero_bit_kernels():\n",
    "    for layer in model.features:\n",
    "        if isinstance(layer, QConv2d):\n",
    "            if (layer.b.view(-1) <= 0).any():\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def restore_zero_bit_kernels(restore_fraction=0.1):  # Reduced restore_fraction from 0.2 to 0.1\n",
    "    for layer in model.features:\n",
    "        if isinstance(layer, QConv2d):\n",
    "            b_flat = layer.b.view(-1)\n",
    "            zero_bit_indices = (b_flat <= 0).nonzero(as_tuple=False).view(-1)\n",
    "            num_restore = int(restore_fraction * len(zero_bit_indices))  # Restore a smaller fraction\n",
    "            if num_restore > 0:\n",
    "                restore_indices = zero_bit_indices[torch.randperm(len(zero_bit_indices))[:num_restore]]\n",
    "                b_flat[restore_indices] = 2.0  # Restore bits to 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:   3.16  bytes: 326192.0  acc: 97.91%  safety_loss: 1.83:   1%|          | 59/10000 [00:07<20:41,  8.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m model_bytes \u001b[38;5;241m=\u001b[39m Q \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m8\u001b[39m \u001b[38;5;241m*\u001b[39m weight_count\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m9\u001b[39m:\n\u001b[0;32m---> 16\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mget_test_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     safety_acc \u001b[38;5;241m=\u001b[39m get_safety_acc()\n\u001b[1;32m     18\u001b[0m     acc_drop \u001b[38;5;241m=\u001b[39m prev_safety_acc \u001b[38;5;241m-\u001b[39m safety_acc\n",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m, in \u001b[0;36mget_test_acc\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(X_test\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m      6\u001b[0m pred \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (pred \u001b[38;5;241m==\u001b[39m \u001b[43mY_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Variables for tracking\n",
    "prev_safety_acc = None\n",
    "# **Increased safety_acc_drop_threshold to allow more compression before restoring kernels**\n",
    "safety_acc_drop_threshold = 15.0  # Increased from 10.0 to 15.0 percentage points\n",
    "test_accs, bytes_used, safety_losses = [], [], []\n",
    "\n",
    "# Initial safety accuracy\n",
    "initial_safety_acc = get_safety_acc()\n",
    "prev_safety_acc = initial_safety_acc\n",
    "\n",
    "# Training loop\n",
    "for i in (t := trange(10000)):\n",
    "    loss, Q, safety_loss = train_step()\n",
    "    model_bytes = Q / 8 * weight_count\n",
    "    if i % 10 == 9:\n",
    "        test_acc = get_test_acc()\n",
    "        safety_acc = get_safety_acc()\n",
    "        acc_drop = prev_safety_acc - safety_acc\n",
    "        if acc_drop > safety_acc_drop_threshold:\n",
    "            if check_zero_bit_kernels():\n",
    "                restore_zero_bit_kernels(restore_fraction=0.1)  # Restore fewer kernels\n",
    "        prev_safety_acc = safety_acc\n",
    "    else:\n",
    "        test_acc = test_accs[-1] if test_accs else 0.0\n",
    "    test_accs.append(test_acc)\n",
    "    bytes_used.append(model_bytes)\n",
    "    safety_losses.append(safety_loss)\n",
    "    t.set_description(f\"loss: {loss:6.2f}  bytes: {model_bytes:.1f}  acc: {test_acc:5.2f}%  safety_loss: {safety_loss:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model \n",
    "torch.save(model.state_dict(), 'modelquantizedfinal.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 99.25%\n",
      "Final Safety Accuracy: 99.00%\n",
      "Initial Safety Accuracy: 9.60%\n",
      "Final Model Size: 224730.32 bytes\n",
      "Total Training Loss: 2.0527\n",
      "Total Q Bits: 20.4649\n"
     ]
    }
   ],
   "source": [
    "# Print final metrics\n",
    "print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"Final Safety Accuracy: {safety_acc:.2f}%\")\n",
    "print(f\"Initial Safety Accuracy: {initial_safety_acc:.2f}%\")\n",
    "print(f\"Final Model Size: {model_bytes:.2f} bytes\")\n",
    "print(f\"Total Training Loss: {loss:.4f}\")\n",
    "print(f\"Total Q Bits: {Q:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
