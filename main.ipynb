{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Notebook serves as an overview of the general methodology to track and help reproduce the core contribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Preservation Set Construction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchcam.methods import GradCAM\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import os  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import math\n",
    "from tqdm import trange\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model or define and train one  \n",
    "#''' model '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_class, model_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model_class().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad-CAM Intensity Calculation\n",
    "\n",
    "Grad-CAM (Gradient-weighted Class Activation Mapping) helps in visualizing which parts of the input image are influencing the model's prediction the most. By applying Grad-CAM, we compute activation maps that highlight important regions in the image for a given class. This allows us to rank and select images based on the intensity of these activation maps, ensuring we focus on the most critical examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradcam_intensity(images, labels, model, cam_layer='specify the layer to cam at'):\n",
    "    device = next(model.parameters()).device\n",
    "    activations = []\n",
    "    \n",
    "    with GradCAM(model, target_layer=cam_layer) as cam_extractor:\n",
    "        model.train()\n",
    "        for image, label in zip(images, labels):\n",
    "            image = image.to(device).requires_grad_(True)\n",
    "            with torch.set_grad_enabled(True):\n",
    "                output = model(image.unsqueeze(0))\n",
    "                prediction = output.argmax(dim=1).item()\n",
    "                cam = cam_extractor(prediction, output)\n",
    "                intensity = cam[0].sum().item()\n",
    "                activations.append((image.cpu().detach(), intensity, label))\n",
    "    \n",
    "    return activations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty Sampling\n",
    "\n",
    "Uncertainty sampling is a technique used to select examples for which the model is least confident in its predictions. This is crucial for creating a robust dataset, as it focuses on examples where the model might make mistakes. By selecting the images with high uncertainty (low confidence), we gather examples that help refine and improve the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertain_examples(images, labels, model, threshold=\"hyperparameter to search\"): \n",
    "    device = next(model.parameters()).device\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    uncertain_examples = []\n",
    "    uncertain_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        uncertainties = 1 - probabilities.max(dim=1)[0]\n",
    "        for i, uncertainty in enumerate(uncertainties):\n",
    "            if uncertainty > threshold:\n",
    "                uncertain_examples.append(images[i].cpu())\n",
    "                uncertain_labels.append(labels[i].cpu())\n",
    "    return uncertain_examples, uncertain_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering-Based Projection\n",
    "\n",
    "Clustering-based projection groups examples into clusters based on feature embeddings. This ensures that the selected examples represent a diverse range of images, minimizing redundancy in the dataset. By applying clustering, we can ensure that the dataset contains varied instances from different regions of the feature space, enhancing the overall quality of the selection process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, image):  \n",
    "    # or follow any conventianal method to extract features from the model, for example this will differ for CNN and attention blocks\n",
    "    device = next(model.parameters()).device\n",
    "    image = image.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model.conv3(model.relu(model.conv2(model.relu(model.conv1(image.unsqueeze(0))))))\n",
    "        return output.view(output.size(0), -1)\n",
    "\n",
    "def get_diverse_examples(images, labels, model, num_clusters=10): \n",
    "    #sampling diverse examples from the dataset\n",
    "    embeddings = []\n",
    "    images_list = []\n",
    "    labels_list = []\n",
    "    for image, label in zip(images, labels):\n",
    "        embedding = get_embedding(model, image)\n",
    "        embeddings.append(embedding.squeeze().cpu().numpy())\n",
    "        images_list.append(image.cpu())\n",
    "        labels_list.append(label.cpu())\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    kmeans = KMeans(n_clusters=min(num_clusters, len(embeddings)))\n",
    "    clusters = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    selected_images = []\n",
    "    selected_labels = []\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_indices = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        if cluster_indices:\n",
    "            selected_images.append(images_list[cluster_indices[0]])\n",
    "            selected_labels.append(labels_list[cluster_indices[0]])\n",
    "    \n",
    "    return selected_images, selected_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort and Select Top examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_examples(testloader, model, num_examples=\"hyperparameter to search\"):\n",
    "    all_activations = []\n",
    "    all_uncertain = []\n",
    "    all_diverse = []\n",
    "    all_labels = []\n",
    "\n",
    "    for images, labels in testloader:\n",
    "        activations = compute_gradcam_intensity(images, labels, model)\n",
    "        all_activations.extend(activations)\n",
    "        \n",
    "        uncertain_examples, uncertain_labels = get_uncertain_examples(images, labels, model)\n",
    "        all_uncertain.extend(uncertain_examples)\n",
    "        all_labels.extend(uncertain_labels)\n",
    "        \n",
    "        diverse_examples, diverse_labels = get_diverse_examples(images, labels, model)\n",
    "        all_diverse.extend(diverse_examples)\n",
    "        all_labels.extend(diverse_labels)\n",
    "    \n",
    "    sorted_activations = sorted(all_activations, key=lambda x: x[1], reverse=True)\n",
    "    top_activations = sorted_activations[:num_examples // 2]\n",
    "    top_examples_by_gradcam = [img for img, _, _ in top_activations]\n",
    "    labels_by_gradcam = [label for _, _, label in top_activations]\n",
    "    \n",
    "    combined_examples = top_examples_by_gradcam + all_uncertain + all_diverse\n",
    "    combined_labels = labels_by_gradcam + all_labels\n",
    "    \n",
    "    unique_dict = {}\n",
    "    for img, label in zip(combined_examples, combined_labels):\n",
    "        key = tuple(img.numpy().flatten())\n",
    "        if key not in unique_dict:\n",
    "            unique_dict[key] = (img, label)\n",
    "    \n",
    "    unique_items = list(unique_dict.values())\n",
    "    unique_images = [item[0] for item in unique_items]\n",
    "    unique_labels = [item[1] for item in unique_items]\n",
    "    \n",
    "    if len(unique_images) > num_examples:\n",
    "        unique_images = unique_images[:num_examples]\n",
    "        unique_labels = unique_labels[:num_examples]\n",
    "    \n",
    "    return unique_images, unique_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN _ MNIST Modeling Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation and Transformation\n",
    "\n",
    "The following block of code defines two sets of transformations for the MNIST dataset. The first transformation (`transform`) is applied to both the training and testing data, converting the images into tensors and normalizing them using the mean and standard deviation of the MNIST dataset.\n",
    "\n",
    "The second transformation (`safety_transform`) is designed for the safety set and includes augmentation techniques such as random rotation and random affine translation. These augmentations ensure the diversity of the dataset by introducing slight variations in the images, which help improve the robustness of the model during training.\n",
    "\n",
    "After defining the transformations, the MNIST dataset is loaded for both the training and testing sets. The entire dataset is then converted into tensors, `X_train` and `Y_train` for the training set, and `X_test` and `Y_test` for the test set, by using data loaders that load the entire dataset in one batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:15<00:00, 654026.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 321930.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:02<00:00, 553926.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 6348060.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "safety_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "X_train, Y_train = next(iter(torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=len(train_dataset))))\n",
    "X_test, Y_test = next(iter(torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=len(test_dataset))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized Convolutional Layer\n",
    "\n",
    "This block of code defines a custom quantized convolutional layer `QConv2d`, which applies quantization to the convolution weights. The class is designed to reduce the precision of the weights, effectively simulating a form of quantization during training. The key components are:\n",
    "\n",
    "1. **Initialization**: The weights are initialized with a uniform distribution scaled by the inverse square root of the input size, ensuring that weight magnitudes are appropriately sized. Two additional parameters, `e` and `b`, are initialized. The parameter `e` controls the scaling of the weights, while `b` controls the number of bits used for quantization.\n",
    "\n",
    "2. **qbits Function**: This function computes the total number of quantized bits used by the weights. It sums the `b` values (after applying a ReLU operation to ensure positivity) and multiplies by the number of elements in the first dimension of the weights. This gives a measure of the bit precision for the quantized weights.\n",
    "\n",
    "3. **qweight Function**: This function calculates the quantized version of the weights. It:\n",
    "    - Ensures the bits in `b` are non-negative using ReLU.\n",
    "    - Defines the quantization range based on `b` values (`min_val` and `max_val`).\n",
    "    - Scales the original weights by `2 ** -self.e` to ensure appropriate scaling for quantization.\n",
    "    - Clips the scaled weights between `min_val` and `max_val`, simulating quantization.\n",
    "\n",
    "4. **forward Function**: During the forward pass, the weights are quantized using the `qweight` function. A \"straight-through estimator\" is applied, which rounds the weights but allows gradient flow through the non-quantized weights, preserving differentiability. The convolution is then performed using the quantized weights scaled by `2 ** self.e`.\n",
    "\n",
    "This layer is useful for simulating quantization during training, which can help reduce model size and improve efficiency, especially in hardware-constrained environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(QConv2d, self).__init__()\n",
    "        self.kernel_size = (kernel_size, kernel_size) if isinstance(\n",
    "            kernel_size, int) else tuple(kernel_size)\n",
    "        scale = 1 / math.sqrt(in_channels * math.prod(self.kernel_size))\n",
    "        self.weight = nn.Parameter(torch.empty(\n",
    "            out_channels, in_channels, *self.kernel_size).uniform_(-scale, scale))\n",
    "        self.e = nn.Parameter(torch.full((out_channels, 1, 1, 1), -8.))\n",
    "        self.b = nn.Parameter(torch.full((out_channels, 1, 1, 1), 32.))  # Start with 32 bits\n",
    "\n",
    "    def qbits(self):\n",
    "        return self.b.relu().sum() * self.weight[0].numel()\n",
    "\n",
    "    def qweight(self):\n",
    "        b_rel = self.b.relu()\n",
    "        min_val = torch.where(b_rel > 0, -2 ** (b_rel - 1), torch.zeros_like(b_rel))\n",
    "        max_val = torch.where(b_rel > 0, 2 ** (b_rel - 1) - 1, torch.zeros_like(b_rel))\n",
    "        scaled_weight = 2 ** -self.e * self.weight\n",
    "        qweight = torch.max(torch.min(scaled_weight, max_val), min_val)\n",
    "        return qweight\n",
    "\n",
    "    def forward(self, x):\n",
    "        qw = self.qweight()\n",
    "        w = (qw.round() - qw).detach() + qw  # Straight-through estimator\n",
    "        return nn.functional.conv2d(x, 2 ** self.e * w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model Using Quantized Convolutional Layers\n",
    "\n",
    "This block defines a CNN model `Model` that utilizes the previously defined quantized convolutional layers (`QConv2d`). The architecture follows a standard CNN design but incorporates quantization techniques within the convolutional layers to reduce precision in the weights, improving model efficiency and size reduction.\n",
    "\n",
    "1. **Model Architecture**:\n",
    "    - The model is divided into two main parts: `features` and `classifier`.\n",
    "    - The `features` part contains sequential layers of `QConv2d` (quantized convolutional layers), ReLU activations, Batch Normalization, and Max Pooling. The convolutional layers use quantization to limit precision, with ReLU as the activation function. Batch Normalization without affine parameters or running statistics is applied to normalize the feature maps.\n",
    "    - The `classifier` consists of a single fully connected layer that maps the flattened output of the `features` section to 10 output classes, suitable for classification tasks.\n",
    "\n",
    "2. **Forward Function**:\n",
    "    - The input tensor `x` passes through the `features` section, which applies the sequence of quantized convolutional, activation, normalization, and pooling layers.\n",
    "    - The output is then flattened using `x.view()` to prepare it for the fully connected `classifier` layer, which outputs class scores.\n",
    "\n",
    "3. **qbits Function**:\n",
    "    - The `qbits` function calculates the total number of quantized bits used by all `QConv2d` layers in the `features` section. This function iterates over the layers in `features`, checks if the layer is a quantized convolution (`QConv2d`), and sums the bit usage across all such layers. This helps track how many bits are used by the quantized layers during training.\n",
    "\n",
    "This model is particularly useful for applications where resource efficiency is critical, such as edge devices, as it combines the power of CNNs with the memory and computational benefits of quantization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            QConv2d(1, 32, 5), nn.ReLU(),\n",
    "            QConv2d(32, 32, 5), nn.ReLU(),\n",
    "            nn.BatchNorm2d(32, affine=False, track_running_stats=False),\n",
    "            nn.MaxPool2d(2),\n",
    "            QConv2d(32, 64, 3), nn.ReLU(),\n",
    "            QConv2d(64, 64, 3), nn.ReLU(),\n",
    "            nn.BatchNorm2d(64, affine=False, track_running_stats=False),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Linear(64 * 3 * 3, 10)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def qbits(self):\n",
    "        return sum(l.qbits() for l in self.features if isinstance(l, QConv2d))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset Class: SafetySetDataset\n",
    "\n",
    "The `SafetySetDataset` class is a custom dataset designed to load images and their corresponding labels from a directory. It extends `torch.utils.data.Dataset`, which allows for easy integration with PyTorch's data loading utilities. This dataset is used to handle images saved in the safety set folder.\n",
    "\n",
    "1. **Initialization (`__init__` Method)**:\n",
    "    - The class is initialized with a `safety_set_path` (path to the folder containing images) and an optional `transform` parameter.\n",
    "    - It loops through all `.png` files in the specified folder and extracts both the image paths and the labels. The label is inferred from the file name, assuming that it is in the format `image_{index}_label_{label}.png`.\n",
    "    - The image paths and labels are stored in lists (`self.image_paths` and `self.labels`).\n",
    "\n",
    "2. **Length (`__len__` Method)**:\n",
    "    - This method returns the number of examples in the dataset, which is simply the length of the `self.labels` list.\n",
    "\n",
    "3. **Getting an Item (`__getitem__` Method)**:\n",
    "    - The `__getitem__` method retrieves a single image and its corresponding label based on the index `idx`.\n",
    "    - The image is loaded using `PIL.Image` and converted to grayscale using `.convert('L')`.\n",
    "    - If a transformation is provided (`self.transform`), it is applied to the image. If not, the default transformation is applied using `transform(image)`.\n",
    "    - The method returns a tuple of the transformed image and its corresponding label.\n",
    "\n",
    "This custom dataset class allows for easy loading and transformation of images from the safety set, making it compatible with PyTorch's `DataLoader` for batching and iterating over data during model training or evaluation, accomodate this to an algorithm to work on your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafetySetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, safety_set_path, transform=None):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        for file in os.listdir(safety_set_path):\n",
    "            if file.endswith('.png'):\n",
    "                label_str = file.split('_label_')[-1].split('.png')[0]\n",
    "                label = int(label_str)\n",
    "                image_path = os.path.join(safety_set_path, file)\n",
    "                self.image_paths.append(image_path)\n",
    "                self.labels.append(label)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transform(image)  \n",
    "        label = self.labels[idx]\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model().to(device)\n",
    "opt = optim.Adam(model.parameters())\n",
    "weight_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "safety_set_path = \"/home/mohammad/safety_set_images_d\"  # Update this path if necessary\n",
    "safety_dataset = SafetySetDataset(safety_set_path, transform=safety_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Step Function with Adjusted Compression and Safety Weights\n",
    "\n",
    "This training step function trains the model on both the main training data and a safety dataset while balancing accuracy, compression, and safety using regularization weights.\n",
    "\n",
    "1. **Model Training**:\n",
    "    - The model is set to training mode (`model.train()`), and the optimizer gradient is reset using `opt.zero_grad()`.\n",
    "    - A random batch of samples (512 - the results of the paper are averaged) is selected from the training data (`X_train`) and passed through the model to get the outputs.\n",
    "    - The main loss function, cross-entropy, is calculated between the model's predictions and the actual labels (`Y_train`).\n",
    "\n",
    "2. **Quantization Regularization**:\n",
    "    - The number of quantized bits (`Q`) used by the model is computed using the `qbits()` function.\n",
    "    - A compression weight (`compression_weight`) of **0.1** (hyperparameter to search for) is applied to penalize the model based on the number of bits used. This encourages the model to reduce its bit usage, balancing compression and accuracy.\n",
    "    - The total loss is updated to include the compression penalty, combining both cross-entropy loss and the regularization term.\n",
    "\n",
    "3. **Safety Set Penalty**:\n",
    "    - A random batch of images and labels (64) is sampled from the safety dataset.\n",
    "    - The images are transformed using augmentation, stacked into a batch, and passed through the model to obtain the predictions.\n",
    "    - A separate cross-entropy loss is calculated between the model's predictions and the true labels from the safety set.\n",
    "    - A **safety weight** of **0.05** (hyperparameter to search for) is applied to this safety loss. This regularization term encourages the model to maintain performance on critical safety examples while still prioritizing compression and accuracy.\n",
    "\n",
    "4. **Loss Backpropagation**:\n",
    "    - The total loss, which now includes both the main loss and the safety loss, is backpropagated through the model to compute gradients.\n",
    "    - The optimizer steps to update the model's weights.\n",
    "\n",
    "This function balances the trade-offs between model accuracy, compression (through quantization), and performance on the safety set, allowing for more efficient training while preserving key performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    samples = torch.randint(0, X_train.shape[0], (512,))\n",
    "    outputs = model(X_train[samples].to(device))\n",
    "    loss = nn.functional.cross_entropy(outputs, Y_train[samples].to(device))\n",
    "    Q = model.qbits() / weight_count\n",
    "    compression_weight = 0.1  # Adjusted from 0.05 to 0.1\n",
    "    loss = loss + compression_weight * Q  # Hyperparameter determines compression vs accuracy\n",
    "\n",
    "    # Safety set penalty\n",
    "    # Randomly sample from safety dataset with augmentation\n",
    "    safety_indices = torch.randint(0, len(safety_dataset), (64,))\n",
    "    safety_images_batch = []\n",
    "    safety_labels_batch = []\n",
    "    for idx in safety_indices:\n",
    "        img, label = safety_dataset[idx]\n",
    "        safety_images_batch.append(img)\n",
    "        safety_labels_batch.append(label)\n",
    "    safety_images_batch = torch.stack(safety_images_batch).to(device)\n",
    "    safety_labels_batch = torch.tensor(safety_labels_batch).to(device)\n",
    "    safety_outputs = model(safety_images_batch)\n",
    "    safety_loss = nn.functional.cross_entropy(safety_outputs, safety_labels_batch)\n",
    "    safety_weight = 0.05  \n",
    "    loss = loss + safety_weight * safety_loss\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    return loss.item(), Q.item(), safety_loss.item()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
