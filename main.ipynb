{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Notebook serves as an overview of the general methodology to track and help reproduce the core contribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Preservation Set Construction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchcam.methods import GradCAM\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import os  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import math\n",
    "from tqdm import trange\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model or define and train one  \n",
    "#''' model '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_class, model_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model_class().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad-CAM Intensity Calculation\n",
    "\n",
    "Grad-CAM (Gradient-weighted Class Activation Mapping) helps in visualizing which parts of the input image are influencing the model's prediction the most. By applying Grad-CAM, we compute activation maps that highlight important regions in the image for a given class. This allows us to rank and select images based on the intensity of these activation maps, ensuring we focus on the most critical examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradcam_intensity(images, labels, model, cam_layer='specify the layer to cam at'):\n",
    "    device = next(model.parameters()).device\n",
    "    activations = []\n",
    "    \n",
    "    with GradCAM(model, target_layer=cam_layer) as cam_extractor:\n",
    "        model.train()\n",
    "        for image, label in zip(images, labels):\n",
    "            image = image.to(device).requires_grad_(True)\n",
    "            with torch.set_grad_enabled(True):\n",
    "                output = model(image.unsqueeze(0))\n",
    "                prediction = output.argmax(dim=1).item()\n",
    "                cam = cam_extractor(prediction, output)\n",
    "                intensity = cam[0].sum().item()\n",
    "                activations.append((image.cpu().detach(), intensity, label))\n",
    "    \n",
    "    return activations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty Sampling\n",
    "\n",
    "Uncertainty sampling is a technique used to select examples for which the model is least confident in its predictions. This is crucial for creating a robust dataset, as it focuses on examples where the model might make mistakes. By selecting the images with high uncertainty (low confidence), we gather examples that help refine and improve the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertain_examples(images, labels, model, threshold=\"hyperparameter to search\"): \n",
    "    device = next(model.parameters()).device\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    uncertain_examples = []\n",
    "    uncertain_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        uncertainties = 1 - probabilities.max(dim=1)[0]\n",
    "        for i, uncertainty in enumerate(uncertainties):\n",
    "            if uncertainty > threshold:\n",
    "                uncertain_examples.append(images[i].cpu())\n",
    "                uncertain_labels.append(labels[i].cpu())\n",
    "    return uncertain_examples, uncertain_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering-Based Projection\n",
    "\n",
    "Clustering-based projection groups examples into clusters based on feature embeddings. This ensures that the selected examples represent a diverse range of images, minimizing redundancy in the dataset. By applying clustering, we can ensure that the dataset contains varied instances from different regions of the feature space, enhancing the overall quality of the selection process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, image):  \n",
    "    # or follow any conventianal method to extract features from the model, for example this will differ for CNN and attention blocks\n",
    "    device = next(model.parameters()).device\n",
    "    image = image.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model.conv3(model.relu(model.conv2(model.relu(model.conv1(image.unsqueeze(0))))))\n",
    "        return output.view(output.size(0), -1)\n",
    "\n",
    "def get_diverse_examples(images, labels, model, num_clusters=10): \n",
    "    #sampling diverse examples from the dataset\n",
    "    embeddings = []\n",
    "    images_list = []\n",
    "    labels_list = []\n",
    "    for image, label in zip(images, labels):\n",
    "        embedding = get_embedding(model, image)\n",
    "        embeddings.append(embedding.squeeze().cpu().numpy())\n",
    "        images_list.append(image.cpu())\n",
    "        labels_list.append(label.cpu())\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    kmeans = KMeans(n_clusters=min(num_clusters, len(embeddings)))\n",
    "    clusters = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    selected_images = []\n",
    "    selected_labels = []\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_indices = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        if cluster_indices:\n",
    "            selected_images.append(images_list[cluster_indices[0]])\n",
    "            selected_labels.append(labels_list[cluster_indices[0]])\n",
    "    \n",
    "    return selected_images, selected_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort and Select Top examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_examples(testloader, model, num_examples=\"hyperparameter to search\"):\n",
    "    all_activations = []\n",
    "    all_uncertain = []\n",
    "    all_diverse = []\n",
    "    all_labels = []\n",
    "\n",
    "    for images, labels in testloader:\n",
    "        activations = compute_gradcam_intensity(images, labels, model)\n",
    "        all_activations.extend(activations)\n",
    "        \n",
    "        uncertain_examples, uncertain_labels = get_uncertain_examples(images, labels, model)\n",
    "        all_uncertain.extend(uncertain_examples)\n",
    "        all_labels.extend(uncertain_labels)\n",
    "        \n",
    "        diverse_examples, diverse_labels = get_diverse_examples(images, labels, model)\n",
    "        all_diverse.extend(diverse_examples)\n",
    "        all_labels.extend(diverse_labels)\n",
    "    \n",
    "    sorted_activations = sorted(all_activations, key=lambda x: x[1], reverse=True)\n",
    "    top_activations = sorted_activations[:num_examples // 2]\n",
    "    top_examples_by_gradcam = [img for img, _, _ in top_activations]\n",
    "    labels_by_gradcam = [label for _, _, label in top_activations]\n",
    "    \n",
    "    combined_examples = top_examples_by_gradcam + all_uncertain + all_diverse\n",
    "    combined_labels = labels_by_gradcam + all_labels\n",
    "    \n",
    "    unique_dict = {}\n",
    "    for img, label in zip(combined_examples, combined_labels):\n",
    "        key = tuple(img.numpy().flatten())\n",
    "        if key not in unique_dict:\n",
    "            unique_dict[key] = (img, label)\n",
    "    \n",
    "    unique_items = list(unique_dict.values())\n",
    "    unique_images = [item[0] for item in unique_items]\n",
    "    unique_labels = [item[1] for item in unique_items]\n",
    "    \n",
    "    if len(unique_images) > num_examples:\n",
    "        unique_images = unique_images[:num_examples]\n",
    "        unique_labels = unique_labels[:num_examples]\n",
    "    \n",
    "    return unique_images, unique_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN _ MNIST Modeling Approach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
