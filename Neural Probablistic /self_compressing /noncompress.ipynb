{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Download the Punkt tokenizer models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "\n",
    "# Define the NPLM architecture\n",
    "class NPLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n",
    "        super(NPLM, self).__init__()\n",
    "        # Embedding layer (lookup table)\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # First fully connected layer followed by tanh\n",
    "        self.fc1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Output layer (softmax over the vocabulary)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        # Look up the embeddings for the context words\n",
    "        embeds = self.embeddings(context).view((1, -1))  # Flatten the embeddings\n",
    "        \n",
    "        # Pass through the first hidden layer with Tanh activation\n",
    "        hidden = torch.tanh(self.fc1(embeds))\n",
    "        \n",
    "        # Compute output with the softmax layer\n",
    "        output = self.fc2(hidden)\n",
    "        return F.log_softmax(output, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "\n",
    "# Define the NPLM architecture\n",
    "class NPLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n",
    "        super(NPLM, self).__init__()\n",
    "        # Embedding layer (lookup table)\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # First fully connected layer followed by tanh\n",
    "        self.fc1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Output layer (softmax over the vocabulary)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        # Look up the embeddings for the context words\n",
    "        embeds = self.embeddings(context).view((1, -1))  # Flatten the embeddings\n",
    "        \n",
    "        # Pass through the first hidden layer with Tanh activation\n",
    "        hidden = torch.tanh(self.fc1(embeds))\n",
    "        \n",
    "        # Compute output with the softmax layer\n",
    "        output = self.fc2(hidden)\n",
    "        return F.log_softmax(output, dim=-1)\n",
    "\n",
    "# Load SST2 dataset from the local file (directly using words without tokenization)\n",
    "file_path = '/home/mohammad/Safety-Driven-Self-Compressing-Neural-Networks/Neural Probablistic /data/sst2_train.txt'\n",
    "\n",
    "data = []\n",
    "\n",
    "# Read the file and split it into words directly\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        label, sentence = line.strip().split('\\t')\n",
    "        words = sentence.split()  # Split the sentence into words\n",
    "        data.append((label, words))\n",
    "\n",
    "# Build a vocabulary from the words\n",
    "def build_vocab(data):\n",
    "    vocab = Counter()\n",
    "    for _, words in data:\n",
    "        vocab.update(words)\n",
    "    return {word: idx for idx, (word, _) in enumerate(vocab.items(), start=0)}\n",
    "\n",
    "# Create a vocabulary from the SST2 dataset\n",
    "vocab = build_vocab(data)\n",
    "\n",
    "# Create context-target pairs (3 words as context, next word as target)\n",
    "def create_context_target_pairs(data, context_size):\n",
    "    pairs = []\n",
    "    for _, words in data:\n",
    "        for i in range(len(words) - context_size):\n",
    "            context = words[i:i + context_size]\n",
    "            target = words[i + context_size]\n",
    "            pairs.append((context, target))\n",
    "    return pairs\n",
    "\n",
    "# Create the training data\n",
    "train_data = create_context_target_pairs(data, context_size=3)\n",
    "\n",
    "# Convert words to indices using the vocabulary\n",
    "def words_to_indices(context, vocab):\n",
    "    return torch.tensor([vocab[word] for word in context], dtype=torch.long)\n",
    "\n",
    "# Define hyperparameters\n",
    "embedding_dim = 50  # Embedding dimension\n",
    "hidden_dim = 100    # Hidden layer size\n",
    "context_size = 3    # Context of 3 words\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Instantiate the model\n",
    "model = NPLM(vocab_size=vocab_size, embedding_dim=embedding_dim, context_size=context_size, hidden_dim=hidden_dim)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the NPLM model\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in train_data:\n",
    "        # Convert context words and target to indices\n",
    "        context_idxs = words_to_indices(context, vocab)\n",
    "        target_idx = torch.tensor([vocab[target]], dtype=torch.long)\n",
    "\n",
    "        # Zero gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Compute loss and backward pass\n",
    "        loss = loss_function(log_probs, target_idx)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss/len(train_data)}')\n",
    "\n",
    "# Example usage: Predict the next word given a context of 3 words\n",
    "with torch.no_grad():\n",
    "    test_context = ['i', 'love', 'this']  # Replace with any context from SST2\n",
    "    test_context_idxs = words_to_indices(test_context, vocab)\n",
    "    prediction = model(test_context_idxs)\n",
    "    predicted_word_idx = prediction.argmax(dim=-1).item()\n",
    "    predicted_word = list(vocab.keys())[list(vocab.values()).index(predicted_word_idx)]\n",
    "    print(f\"Given context: {' '.join(test_context)}, predicted next word: {predicted_word}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
